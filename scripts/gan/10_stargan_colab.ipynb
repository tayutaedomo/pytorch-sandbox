{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_stargan_colab.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1weBFmxPAo5hA2I2cpag2LcBZ-B2VMNbU","authorship_tag":"ABX9TyMQXzI+Y2uxG28S7gvr8Vhg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hdXTZWxdRlr4"},"source":["## StarGAN PyTorch実装を学ぶ\n","- Reference\n","  - https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/stargan\n","  "]},{"cell_type":"code","metadata":{"id":"wNzTRQlrRdQv","executionInfo":{"status":"ok","timestamp":1603707788820,"user_tz":-540,"elapsed":1288,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["import os\n","import sys\n","import datetime\n","import time\n","import numpy as np\n","import pandas as pd\n","import itertools"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcuP0KtLR1sZ","executionInfo":{"status":"ok","timestamp":1603707793263,"user_tz":-540,"elapsed":5724,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["import torchvision.transforms as transforms\n","from torchvision.utils import save_image, make_grid"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-Frrihw_qCD","executionInfo":{"status":"ok","timestamp":1603707793264,"user_tz":-540,"elapsed":5721,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","import torch.autograd as autograd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxFYcNcZ_u63","executionInfo":{"status":"ok","timestamp":1603707793265,"user_tz":-540,"elapsed":5718,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["# for model\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"cymjIFCBWngT","executionInfo":{"status":"ok","timestamp":1603707793266,"user_tz":-540,"elapsed":5713,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["# for datasets\n","\n","import glob\n","import random\n","#import os\n","#import numpy as np\n","\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import torchvision.transforms as transforms"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYW7Q1oD_xr5","executionInfo":{"status":"ok","timestamp":1603707793267,"user_tz":-540,"elapsed":5710,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["#torch.__version__"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJaCA6PT_0JC","executionInfo":{"status":"ok","timestamp":1603707793268,"user_tz":-540,"elapsed":5707,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["DATA_DIR_PATH = '/content/drive/My Drive/project/ML/pytorch-gan/data'\n","\n","OUTPUT_DIR_PATH = os.path.join(DATA_DIR_PATH, '10_out')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"3I4pzsl1AugF","executionInfo":{"status":"ok","timestamp":1603707795167,"user_tz":-540,"elapsed":7602,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["os.makedirs(os.path.join(OUTPUT_DIR_PATH, 'images'), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR_PATH, 'saved_models'), exist_ok=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKXltr83uxcK","executionInfo":{"status":"ok","timestamp":1603707795168,"user_tz":-540,"elapsed":7599,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","\n","\n","##############################\n","#           RESNET\n","##############################\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_features):\n","        super(ResidualBlock, self).__init__()\n","\n","        conv_block = [\n","            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n","        ]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class GeneratorResNet(nn.Module):\n","    def __init__(self, img_shape=(3, 128, 128), res_blocks=9, c_dim=5):\n","        super(GeneratorResNet, self).__init__()\n","        channels, img_size, _ = img_shape\n","\n","        # Initial convolution block\n","        model = [\n","            nn.Conv2d(channels + c_dim, 64, 7, stride=1, padding=3, bias=False),\n","            nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","        ]\n","\n","        # Downsampling\n","        curr_dim = 64\n","        for _ in range(2):\n","            model += [\n","                nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=1, bias=False),\n","                nn.InstanceNorm2d(curr_dim * 2, affine=True, track_running_stats=True),\n","                nn.ReLU(inplace=True),\n","            ]\n","            curr_dim *= 2\n","\n","        # Residual blocks\n","        for _ in range(res_blocks):\n","            model += [ResidualBlock(curr_dim)]\n","\n","        # Upsampling\n","        for _ in range(2):\n","            model += [\n","                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, stride=2, padding=1, bias=False),\n","                nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n","                nn.ReLU(inplace=True),\n","            ]\n","            curr_dim = curr_dim // 2\n","\n","        # Output layer\n","        model += [nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), nn.Tanh()]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x, c):\n","        c = c.view(c.size(0), c.size(1), 1, 1)\n","        c = c.repeat(1, 1, x.size(2), x.size(3))\n","        x = torch.cat((x, c), 1)\n","        return self.model(x)\n","\n","\n","##############################\n","#        Discriminator\n","##############################\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, img_shape=(3, 128, 128), c_dim=5, n_strided=6):\n","        super(Discriminator, self).__init__()\n","        channels, img_size, _ = img_shape\n","\n","        def discriminator_block(in_filters, out_filters):\n","            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n","            return layers\n","\n","        layers = discriminator_block(channels, 64)\n","        curr_dim = 64\n","        for _ in range(n_strided - 1):\n","            layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n","            curr_dim *= 2\n","\n","        self.model = nn.Sequential(*layers)\n","\n","        # Output 1: PatchGAN\n","        self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n","        # Output 2: Class prediction\n","        kernel_size = img_size // 2 ** n_strided\n","        self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n","\n","    def forward(self, img):\n","        feature_repr = self.model(img)\n","        out_adv = self.out1(feature_repr)\n","        out_cls = self.out2(feature_repr)\n","        return out_adv, out_cls.view(out_cls.size(0), -1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmnjU1JXEJoJ","executionInfo":{"status":"ok","timestamp":1603707795169,"user_tz":-540,"elapsed":7596,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["class Option:\n","    def __init__(self):\n","        self.epoch = 0\n","        #self.n_epochs = 200\n","        self.n_epochs = 5\n","        #self.dataset_name = \"img_align_celeba\"\n","        self.dataset_name = \"celeba\"\n","        self.batch_size = 16\n","        self.lr = 0.0002\n","        self.b1 = 0.5\n","        self.b2 = 0.999\n","        self.decay_epoch = 100\n","        self.n_cpu = 8\n","        self.img_height = 128\n","        self.img_width = 128\n","        self.channels = 3\n","        self.sample_interval = 400\n","        self.checkpoint_interval = -1\n","        self.residual_blocks = 6\n","        self.selected_attrs = [\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Male\", \"Young\"]\n","        self.n_critic = 5\n","\n","opt = Option()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yE7OWY51kT7","executionInfo":{"status":"ok","timestamp":1603707795169,"user_tz":-540,"elapsed":7593,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["class CelebADataset(Dataset):\n","    def __init__(self, root, transforms_=None, mode=\"train\", attributes=None):\n","        self.transform = transforms.Compose(transforms_)\n","\n","        self.selected_attrs = attributes\n","        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n","        #self.files = self.files[:-2000] if mode == \"train\" else self.files[-2000:]\n","        self.files = self.files[:-300] if mode == \"train\" else self.files[-300:]\n","        self.label_path = glob.glob(\"%s/*.txt\" % root)[0]\n","        self.annotations = self.get_annotations()\n","\n","    def get_annotations(self):\n","        \"\"\"Extracts annotations for CelebA\"\"\"\n","        annotations = {}\n","        lines = [line.rstrip() for line in open(self.label_path, \"r\")]\n","        self.label_names = lines[1].split()\n","        for _, line in enumerate(lines[2:]):\n","            filename, *values = line.split()\n","            labels = []\n","            for attr in self.selected_attrs:\n","                idx = self.label_names.index(attr)\n","                labels.append(1 * (values[idx] == \"1\"))\n","            annotations[filename] = labels\n","        return annotations\n","\n","    def __getitem__(self, index):\n","        filepath = self.files[index % len(self.files)]\n","        filename = filepath.split(\"/\")[-1]\n","        img = self.transform(Image.open(filepath))\n","        label = self.annotations[filename]\n","        label = torch.FloatTensor(np.array(label))\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.files)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"1bxrWFV4A9cn","executionInfo":{"status":"ok","timestamp":1603707803311,"user_tz":-540,"elapsed":15731,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["IMG_ROOT_PATH = os.path.join(DATA_DIR_PATH, opt.dataset_name)\n","#print(IMG_ROOT_PATH)\n","\n","# Configure dataloaders\n","train_transforms = [\n","    transforms.Resize(int(1.12 * opt.img_height), Image.BICUBIC),\n","    transforms.RandomCrop(opt.img_height),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","]\n","\n","dataloader = DataLoader(\n","    CelebADataset(\n","        IMG_ROOT_PATH, transforms_=train_transforms, mode=\"train\", attributes=opt.selected_attrs\n","    ),\n","    batch_size=opt.batch_size,\n","    shuffle=True,\n","    num_workers=opt.n_cpu,\n",")\n","\n","val_transforms = [\n","    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","]\n","\n","val_dataloader = DataLoader(\n","    CelebADataset(\n","        IMG_ROOT_PATH, transforms_=val_transforms, mode=\"val\", attributes=opt.selected_attrs\n","    ),\n","    batch_size=10,\n","    shuffle=True,\n","    num_workers=1,\n",")"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7Wp-CWWZ3lT","executionInfo":{"status":"ok","timestamp":1603707814348,"user_tz":-540,"elapsed":26764,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["c_dim = len(opt.selected_attrs)\n","img_shape = (opt.channels, opt.img_height, opt.img_width)\n","\n","cuda = torch.cuda.is_available()\n","\n","# Loss functions\n","criterion_cycle = torch.nn.L1Loss()\n","\n","\n","def criterion_cls(logit, target):\n","    return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0)\n","\n","\n","# Loss weights\n","lambda_cls = 1\n","lambda_rec = 10\n","lambda_gp = 10\n","\n","# Initialize generator and discriminator\n","generator = GeneratorResNet(img_shape=img_shape, res_blocks=opt.residual_blocks, c_dim=c_dim)\n","discriminator = Discriminator(img_shape=img_shape, c_dim=c_dim)\n","\n","if cuda:\n","    generator = generator.cuda()\n","    discriminator = discriminator.cuda()\n","    criterion_cycle.cuda()\n","\n","if opt.epoch != 0:\n","    # Load pretrained models\n","    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\" % opt.epoch))\n","    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\" % opt.epoch))\n","else:\n","    generator.apply(weights_init_normal)\n","    discriminator.apply(weights_init_normal)\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZt1ifmeZS2T","executionInfo":{"status":"ok","timestamp":1603707814348,"user_tz":-540,"elapsed":26761,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["# Tensor type\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"P95nomvfwHrb","executionInfo":{"status":"ok","timestamp":1603707814349,"user_tz":-540,"elapsed":26758,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["img_save_dir = os.path.join(OUTPUT_DIR_PATH, 'images')\n","\n","label_changes = [\n","    ((0, 1), (1, 0), (2, 0)),  # Set to black hair\n","    ((0, 0), (1, 1), (2, 0)),  # Set to blonde hair\n","    ((0, 0), (1, 0), (2, 1)),  # Set to brown hair\n","    ((3, -1),),  # Flip gender\n","    ((4, -1),),  # Age flip\n","]\n","\n","\n","def sample_images(batches_done):\n","    \"\"\"Saves a generated sample of domain translations\"\"\"\n","    val_imgs, val_labels = next(iter(val_dataloader))\n","    val_imgs = Variable(val_imgs.type(Tensor))\n","    val_labels = Variable(val_labels.type(Tensor))\n","\n","    img_samples = None\n","\n","    for i in range(10):\n","        img, label = val_imgs[i], val_labels[i]\n","        # Repeat for number of label changes\n","        imgs = img.repeat(c_dim, 1, 1, 1)\n","        labels = label.repeat(c_dim, 1)\n","        # Make changes to labels\n","        for sample_i, changes in enumerate(label_changes):\n","            for col, val in changes:\n","                labels[sample_i, col] = 1 - labels[sample_i, col] if val == -1 else val\n","\n","        # Generate translations\n","        gen_imgs = generator(imgs, labels)\n","        # Concatenate images by width\n","        gen_imgs = torch.cat([x for x in gen_imgs.data], -1)\n","        img_sample = torch.cat((img.data, gen_imgs), -1)\n","        # Add as row to generated samples\n","        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n","\n","    file_name = os.path.join(img_save_dir, '{}.png'.format(batches_done))\n","    #save_image(img_samples.view(1, *img_samples.shape), \"images/%s.png\" % batches_done, normalize=True)\n","    save_image(img_samples.view(1, *img_samples.shape), file_name, normalize=True)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tm_6oLduI89Y","executionInfo":{"status":"ok","timestamp":1603707814350,"user_tz":-540,"elapsed":26752,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["def compute_gradient_penalty(D, real_samples, fake_samples):\n","    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n","    # Random weight term for interpolation between real and fake samples\n","    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n","    # Get random interpolation between real and fake samples\n","    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n","    d_interpolates, _ = D(interpolates)\n","    fake = Variable(Tensor(np.ones(d_interpolates.shape)), requires_grad=False)\n","\n","    # Get gradient w.r.t. interpolates\n","    gradients = autograd.grad(\n","        outputs=d_interpolates,\n","        inputs=interpolates,\n","        grad_outputs=fake,\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True,\n","    )[0]\n","    gradients = gradients.view(gradients.size(0), -1)\n","    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","\n","    return gradient_penalty"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yRnTf-3azfU","executionInfo":{"status":"ok","timestamp":1603707923125,"user_tz":-540,"elapsed":135515,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}},"outputId":"2d202174-85d2-428b-923b-af83185ed83f","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["# ----------\n","#  Training\n","# ----------\n","\n","#saved_samples = []\n","start_time = time.time()\n","\n","for epoch in range(opt.epoch, opt.n_epochs):\n","    for i, (imgs, labels) in enumerate(dataloader):\n","\n","        # Model inputs\n","        imgs = Variable(imgs.type(Tensor))\n","        labels = Variable(labels.type(Tensor))\n","\n","        # Sample labels as generator inputs\n","        sampled_c = Variable(Tensor(np.random.randint(0, 2, (imgs.size(0), c_dim))))\n","        # Generate fake batch of images\n","        fake_imgs = generator(imgs, sampled_c)\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","        # Real images\n","        real_validity, pred_cls = discriminator(imgs)\n","        # Fake images\n","        fake_validity, _ = discriminator(fake_imgs.detach())\n","        # Gradient penalty\n","        gradient_penalty = compute_gradient_penalty(discriminator, imgs.data, fake_imgs.data)\n","        # Adversarial loss\n","        loss_D_adv = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n","        # Classification loss\n","        loss_D_cls = criterion_cls(pred_cls, labels)\n","        # Total loss\n","        loss_D = loss_D_adv + lambda_cls * loss_D_cls\n","\n","        loss_D.backward()\n","        optimizer_D.step()\n","\n","        optimizer_G.zero_grad()\n","\n","        # Every n_critic times update generator\n","        if i % opt.n_critic == 0:\n","\n","            # -----------------\n","            #  Train Generator\n","            # -----------------\n","\n","            # Translate and reconstruct image\n","            gen_imgs = generator(imgs, sampled_c)\n","            recov_imgs = generator(gen_imgs, labels)\n","            # Discriminator evaluates translated image\n","            fake_validity, pred_cls = discriminator(gen_imgs)\n","            # Adversarial loss\n","            loss_G_adv = -torch.mean(fake_validity)\n","            # Classification loss\n","            loss_G_cls = criterion_cls(pred_cls, sampled_c)\n","            # Reconstruction loss\n","            loss_G_rec = criterion_cycle(recov_imgs, imgs)\n","            # Total loss\n","            loss_G = loss_G_adv + lambda_cls * loss_G_cls + lambda_rec * loss_G_rec\n","\n","            loss_G.backward()\n","            optimizer_G.step()\n","\n","            # --------------\n","            #  Log Progress\n","            # --------------\n","\n","            # Determine approximate time left\n","            batches_done = epoch * len(dataloader) + i\n","            batches_left = opt.n_epochs * len(dataloader) - batches_done\n","            time_left = datetime.timedelta(seconds=batches_left * (time.time() - start_time) / (batches_done + 1))\n","\n","            # Print log\n","            sys.stdout.write(\n","                \"\\r[Epoch %d/%d] [Batch %d/%d] [D adv: %f, aux: %f] [G loss: %f, adv: %f, aux: %f, cycle: %f] ETA: %s\"\n","                % (\n","                    epoch,\n","                    opt.n_epochs,\n","                    i,\n","                    len(dataloader),\n","                    loss_D_adv.item(),\n","                    loss_D_cls.item(),\n","                    loss_G.item(),\n","                    loss_G_adv.item(),\n","                    loss_G_cls.item(),\n","                    loss_G_rec.item(),\n","                    time_left,\n","                )\n","            )\n","\n","            # If at sample interval sample and save image\n","            if batches_done % opt.sample_interval == 0:\n","                sample_images(batches_done)\n","\n","    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n","        # Save model checkpoints\n","        torch.save(generator.state_dict(), \"saved_models/generator_%d.pth\" % epoch)\n","        torch.save(discriminator.state_dict(), \"saved_models/discriminator_%d.pth\" % epoch)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["[Epoch 4/5] [Batch 40/44] [D adv: -6.291781, aux: 2.661407] [G loss: 10.671492, adv: 1.206390, aux: 6.250092, cycle: 0.321501] ETA: 0:00:01.993502"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n3SwPW6LbV9V","executionInfo":{"status":"ok","timestamp":1603707923126,"user_tz":-540,"elapsed":135514,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":[""],"execution_count":17,"outputs":[]}]}