{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_stargan_colab.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1weBFmxPAo5hA2I2cpag2LcBZ-B2VMNbU","authorship_tag":"ABX9TyPJCi/9x3vljk00a9KIDIXD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hdXTZWxdRlr4"},"source":["## StarGAN PyTorch実装を学ぶ\n","- Reference\n","  - https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/stargan\n","  "]},{"cell_type":"code","metadata":{"id":"wNzTRQlrRdQv","executionInfo":{"status":"ok","timestamp":1603427618038,"user_tz":-540,"elapsed":1914,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["import os\n","import sys\n","import datetime\n","import time\n","import numpy as np\n","import pandas as pd\n","import itertools"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcuP0KtLR1sZ","executionInfo":{"status":"ok","timestamp":1603427621627,"user_tz":-540,"elapsed":5497,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["import torchvision.transforms as transforms\n","from torchvision.utils import save_image, make_grid"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-Frrihw_qCD","executionInfo":{"status":"ok","timestamp":1603427621628,"user_tz":-540,"elapsed":5495,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","import torch.autograd as autograd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxFYcNcZ_u63","executionInfo":{"status":"ok","timestamp":1603427621628,"user_tz":-540,"elapsed":5492,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["# for model\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"cymjIFCBWngT","executionInfo":{"status":"ok","timestamp":1603427621629,"user_tz":-540,"elapsed":5491,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["# for datasets\n","\n","import glob\n","import random\n","#import os\n","#import numpy as np\n","\n","from torch.utils.data import Dataset\n","from PIL import Image\n","import torchvision.transforms as transforms"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYW7Q1oD_xr5","executionInfo":{"status":"ok","timestamp":1603427621630,"user_tz":-540,"elapsed":5490,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["#torch.__version__"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJaCA6PT_0JC","executionInfo":{"status":"ok","timestamp":1603427621631,"user_tz":-540,"elapsed":5489,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["DATA_DIR_PATH = '/content/drive/My Drive/project/ML/pytorch-gan/data'\n","\n","OUTPUT_DIR_PATH = os.path.join(DATA_DIR_PATH, '10_out')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"3I4pzsl1AugF","executionInfo":{"status":"ok","timestamp":1603427630460,"user_tz":-540,"elapsed":3398,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["os.makedirs(os.path.join(OUTPUT_DIR_PATH, 'images'), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_DIR_PATH, 'saved_models'), exist_ok=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKXltr83uxcK","executionInfo":{"status":"ok","timestamp":1603428190755,"user_tz":-540,"elapsed":853,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","\n","\n","##############################\n","#           RESNET\n","##############################\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_features):\n","        super(ResidualBlock, self).__init__()\n","\n","        conv_block = [\n","            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n","            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n","        ]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        return x + self.conv_block(x)\n","\n","\n","class GeneratorResNet(nn.Module):\n","    def __init__(self, img_shape=(3, 128, 128), res_blocks=9, c_dim=5):\n","        super(GeneratorResNet, self).__init__()\n","        channels, img_size, _ = img_shape\n","\n","        # Initial convolution block\n","        model = [\n","            nn.Conv2d(channels + c_dim, 64, 7, stride=1, padding=3, bias=False),\n","            nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","        ]\n","\n","        # Downsampling\n","        curr_dim = 64\n","        for _ in range(2):\n","            model += [\n","                nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=1, bias=False),\n","                nn.InstanceNorm2d(curr_dim * 2, affine=True, track_running_stats=True),\n","                nn.ReLU(inplace=True),\n","            ]\n","            curr_dim *= 2\n","\n","        # Residual blocks\n","        for _ in range(res_blocks):\n","            model += [ResidualBlock(curr_dim)]\n","\n","        # Upsampling\n","        for _ in range(2):\n","            model += [\n","                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, stride=2, padding=1, bias=False),\n","                nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n","                nn.ReLU(inplace=True),\n","            ]\n","            curr_dim = curr_dim // 2\n","\n","        # Output layer\n","        model += [nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), nn.Tanh()]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x, c):\n","        c = c.view(c.size(0), c.size(1), 1, 1)\n","        c = c.repeat(1, 1, x.size(2), x.size(3))\n","        x = torch.cat((x, c), 1)\n","        return self.model(x)\n","\n","\n","##############################\n","#        Discriminator\n","##############################\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, img_shape=(3, 128, 128), c_dim=5, n_strided=6):\n","        super(Discriminator, self).__init__()\n","        channels, img_size, _ = img_shape\n","\n","        def discriminator_block(in_filters, out_filters):\n","            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n","            return layers\n","\n","        layers = discriminator_block(channels, 64)\n","        curr_dim = 64\n","        for _ in range(n_strided - 1):\n","            layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n","            curr_dim *= 2\n","\n","        self.model = nn.Sequential(*layers)\n","\n","        # Output 1: PatchGAN\n","        self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n","        # Output 2: Class prediction\n","        kernel_size = img_size // 2 ** n_strided\n","        self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n","\n","    def forward(self, img):\n","        feature_repr = self.model(img)\n","        out_adv = self.out1(feature_repr)\n","        out_cls = self.out2(feature_repr)\n","        return out_adv, out_cls.view(out_cls.size(0), -1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmnjU1JXEJoJ","executionInfo":{"status":"ok","timestamp":1603430018413,"user_tz":-540,"elapsed":1286,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["class Option:\n","    def __init__(self):\n","        self.epoch = 0\n","        self.n_epochs = 200\n","        #self.dataset_name = \"img_align_celeba\"\n","        self.dataset_name = \"celeba\"\n","        self.batch_size = 16\n","        self.lr = 0.0002\n","        self.b1 = 0.5\n","        self.b2 = 0.999\n","        self.decay_epoch = 100\n","        self.n_cpu = 8\n","        self.img_height = 128\n","        self.img_width = 128\n","        self.channels = 3\n","        self.sample_interval = 400\n","        self.checkpoint_interval = -1\n","        self.residual_blocks = 6\n","        self.selected_attrs = [\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Male\", \"Young\"]\n","        self.n_critic = 5\n","\n","opt = Option()"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yE7OWY51kT7","executionInfo":{"status":"ok","timestamp":1603430045627,"user_tz":-540,"elapsed":894,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["class CelebADataset(Dataset):\n","    def __init__(self, root, transforms_=None, mode=\"train\", attributes=None):\n","        self.transform = transforms.Compose(transforms_)\n","\n","        self.selected_attrs = attributes\n","        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n","        #self.files = self.files[:-2000] if mode == \"train\" else self.files[-2000:]\n","        self.files = self.files[:-300] if mode == \"train\" else self.files[-300:]\n","        self.label_path = glob.glob(\"%s/*.txt\" % root)[0]\n","        self.annotations = self.get_annotations()\n","\n","    def get_annotations(self):\n","        \"\"\"Extracts annotations for CelebA\"\"\"\n","        annotations = {}\n","        lines = [line.rstrip() for line in open(self.label_path, \"r\")]\n","        self.label_names = lines[1].split()\n","        for _, line in enumerate(lines[2:]):\n","            filename, *values = line.split()\n","            labels = []\n","            for attr in self.selected_attrs:\n","                idx = self.label_names.index(attr)\n","                labels.append(1 * (values[idx] == \"1\"))\n","            annotations[filename] = labels\n","        return annotations\n","\n","    def __getitem__(self, index):\n","        filepath = self.files[index % len(self.files)]\n","        filename = filepath.split(\"/\")[-1]\n","        img = self.transform(Image.open(filepath))\n","        label = self.annotations[filename]\n","        label = torch.FloatTensor(np.array(label))\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.files)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"1bxrWFV4A9cn","executionInfo":{"status":"ok","timestamp":1603430047702,"user_tz":-540,"elapsed":905,"user":{"displayName":"Noboru Koike","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggp8njUb4jc54JrzfM5cQBkfiCdlz6nY17y8-1szg=s64","userId":"05156582812995850159"}}},"source":["IMG_ROOT_PATH = os.path.join(DATA_DIR_PATH, opt.dataset_name)\n","#print(IMG_ROOT_PATH)\n","\n","# Configure dataloaders\n","train_transforms = [\n","    transforms.Resize(int(1.12 * opt.img_height), Image.BICUBIC),\n","    transforms.RandomCrop(opt.img_height),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","]\n","\n","dataloader = DataLoader(\n","    CelebADataset(\n","        IMG_ROOT_PATH, transforms_=train_transforms, mode=\"train\", attributes=opt.selected_attrs\n","    ),\n","    batch_size=opt.batch_size,\n","    shuffle=True,\n","    num_workers=opt.n_cpu,\n",")\n","\n","val_transforms = [\n","    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","]\n","\n","val_dataloader = DataLoader(\n","    CelebADataset(\n","        IMG_ROOT_PATH, transforms_=val_transforms, mode=\"val\", attributes=opt.selected_attrs\n","    ),\n","    batch_size=10,\n","    shuffle=True,\n","    num_workers=1,\n",")"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"P95nomvfwHrb"},"source":["img_save_dir = os.path.join(OUTPUT_DIR_PATH, 'images')\n","\n","def sample_images(batches_done):\n","    \"\"\"Saves a generated sample from the test set\"\"\"\n","    imgs = next(iter(val_dataloader))\n","    G_AB.eval()\n","    G_BA.eval()\n","    real_A = Variable(imgs[\"A\"].type(Tensor))\n","    fake_B = G_AB(real_A)\n","    real_B = Variable(imgs[\"B\"].type(Tensor))\n","    fake_A = G_BA(real_B)\n","    # Arange images along x-axis\n","    real_A = make_grid(real_A, nrow=5, normalize=True)\n","    real_B = make_grid(real_B, nrow=5, normalize=True)\n","    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n","    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n","    # Arange images along y-axis\n","    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n","    #save_image(image_grid, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), normalize=False)\n","    file_name = os.path.join(img_save_dir, '{}.png'.format(batches_done))\n","    save_image(image_grid, file_name, normalize=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tm_6oLduI89Y"},"source":[""],"execution_count":null,"outputs":[]}]}