{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference\n",
    "  - https://www.bigdata-navi.com/aidrops/2611/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super().__init__()\n",
    "        channel = channel_out // 4\n",
    "\n",
    "        # 1x1 の畳み込み\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel,\n",
    "                               kernel_size=(1, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(channel)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # 3x3 の畳み込み\n",
    "        self.conv2 = nn.Conv2d(channel, channel,\n",
    "                               kernel_size=(3, 3),\n",
    "                               padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channel)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # 1x1 の畳み込み\n",
    "        self.conv3 = nn.Conv2d(channel, channel_out,\n",
    "                               kernel_size=(1, 1),\n",
    "                               padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(channel_out)\n",
    "\n",
    "        # skip connection用のチャネル数調整        \n",
    "        self.shortcut = self._shortcut(channel_in, channel_out)\n",
    "        \n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu2(h)\n",
    "        h = self.conv3(h)\n",
    "        h = self.bn3(h)\n",
    "        shortcut = self.shortcut(x)\n",
    "        y = self.relu3(h + shortcut)  # skip connection\n",
    "        return y\n",
    "\n",
    "    def _shortcut(self, channel_in, channel_out):\n",
    "        if channel_in != channel_out:\n",
    "            return self._projection(channel_in, channel_out)\n",
    "        else:\n",
    "            return lambda x: x\n",
    "\n",
    "    def _projection(self, channel_in, channel_out):\n",
    "        return nn.Conv2d(channel_in, channel_out,\n",
    "                         kernel_size=(1, 1),\n",
    "                         padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:]).view(-1, x.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64,\n",
    "                               kernel_size=(7, 7),\n",
    "                               stride=(2, 2),\n",
    "                               padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(3, 3),\n",
    "                                  stride=(2, 2),\n",
    "                                  padding=1)\n",
    "\n",
    "        # Block 1\n",
    "        self.block0 = self._building_block(256, channel_in=64)\n",
    "\n",
    "        self.block1 = nn.ModuleList([\n",
    "            self._building_block(256) for _ in range(2)\n",
    "        ])\n",
    "        self.conv2 = nn.Conv2d(256, 512,\n",
    "                               kernel_size=(1, 1),\n",
    "                               stride=(2, 2))\n",
    "\n",
    "        # Block 2\n",
    "        self.block2 = nn.ModuleList([\n",
    "            self._building_block(512) for _ in range(4)\n",
    "        ])\n",
    "        self.conv3 = nn.Conv2d(512, 1024,\n",
    "                               kernel_size=(1, 1),\n",
    "                               stride=(2, 2))\n",
    "\n",
    "        # Block 3\n",
    "        self.block3 = nn.ModuleList([\n",
    "            self._building_block(1024) for _ in range(6)\n",
    "        ])\n",
    "        self.conv4 = nn.Conv2d(1024, 2048,\n",
    "                               kernel_size=(1, 1),\n",
    "                               stride=(2, 2))\n",
    "\n",
    "        # Block 4\n",
    "        self.block4 = nn.ModuleList([\n",
    "            self._building_block(2048) for _ in range(3)\n",
    "        ])\n",
    "        self.avg_pool = GlobalAvgPool2d()  # TODO: GlobalAvgPool2d\n",
    "        self.fc = nn.Linear(2048, 1000)\n",
    "        self.out = nn.Linear(1000, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h)\n",
    "        h = self.pool1(h)\n",
    "        h = self.block0(h)\n",
    "\n",
    "        for block in self.block1:\n",
    "            h = block(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        for block in self.block2:\n",
    "            h = block(h)\n",
    "\n",
    "        h = self.conv3(h)\n",
    "\n",
    "        for block in self.block3:\n",
    "            h = block(h)\n",
    "\n",
    "        h = self.conv4(h)\n",
    "\n",
    "        for block in self.block4:\n",
    "            h = block(h)\n",
    "\n",
    "        h = self.avg_pool(h)\n",
    "        h = self.fc(h)\n",
    "        h = torch.relu(h)\n",
    "        h = self.out(h)\n",
    "        y = torch.log_softmax(h, dim=-1)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def _building_block(self,\n",
    "                        channel_out,\n",
    "                        channel_in=None):\n",
    "        if channel_in is None:\n",
    "            channel_in = channel_out\n",
    "\n",
    "        return Block(channel_in, channel_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join('data')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=root, download=True, train=True, transform=transform)\n",
    "\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=root, download=True, train=False, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1bfbe8b26e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-1bfbe8b26e22>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/ML/pytorch-sandbox/venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/ML/pytorch-sandbox/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optimizers.Adam(model.parameters(), weight_decay=0.01)\n",
    "\n",
    "\n",
    "def compute_loss(label, pred):\n",
    "    return criterion(pred, label)\n",
    "\n",
    "def train_step(x, t):\n",
    "    model.train()\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, preds\n",
    "\n",
    "def test_step(x, t):\n",
    "    model.eval()\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    return loss, preds\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    test_loss = 0.\n",
    "    test_acc = 0.\n",
    "\n",
    "    for (x, t) in train_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, _ = train_step(x, t)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    for (x, t) in test_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = test_step(x, t)\n",
    "        test_loss += loss.item()\n",
    "        test_acc += accuracy_score(t.tolist(), preds.argmax(dim=-1).tolist())\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    print('Epoch: {}, Valid Cost: {:.3f}, Valid Acc: {:.3f}'.format(\n",
    "        epoch+1,\n",
    "        test_loss,\n",
    "        test_acc\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
